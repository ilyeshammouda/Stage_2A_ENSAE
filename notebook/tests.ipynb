{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install  numdifftools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import random\n",
    "from math import log\n",
    "import matplotlib.pyplot as plt\n",
    "import numdifftools as nd\n",
    "from scipy.optimize import fmin_cobyla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define funcitons that will be useful for algorithms simulations \n",
    "\n",
    "def Rademacher_matrix(d,n):\n",
    "    \"\"\"\n",
    "    This fucntion generates a Rademacher matrix\n",
    "    \"\"\"\n",
    "    return np.random.choice([-1, 1], size=(d,n))\n",
    "\n",
    "\n",
    "\n",
    "def Rademacher_matrix_concatenated(d,n):\n",
    "    \"\"\"\n",
    "    This function generates a Rademacher matrix and add a line of ones\n",
    "    \"\"\"\n",
    "    Z=Rademacher_matrix(d,n)\n",
    "    Last_line_of_ones = np.ones((1, Z.shape[1]))\n",
    "    return (Z,np.concatenate((Z, Last_line_of_ones), axis=0))\n",
    "\n",
    "\n",
    "\n",
    "def Lasso_reg(Y_tilde,Z):\n",
    "    \"\"\"\n",
    "    This function gives the solution to the Lasso regression in a multivariate model\n",
    "    \"\"\"\n",
    "    lasso = linear_model.LassoCV(cv=5)\n",
    "    lasso.fit(Z,Y_tilde )\n",
    "    g = lasso.coef_\n",
    "    u=lasso.intercept_\n",
    "    return(g,u)\n",
    "\n",
    "\n",
    "\n",
    "def random_vector_unit_spehre(d):\n",
    "    random_vector = np.random.normal(size=d)\n",
    "    return(random_vector/np.linalg.norm(random_vector))\n",
    "\n",
    "\n",
    "\n",
    "def random_unit_vector_on_S_t(d,S_t):\n",
    "     \"\"\"\n",
    "     This function simulates a unit random vector on the spehre S_t\n",
    "     \"\"\"\n",
    "     u_t=np.zeros(d)\n",
    "     indices=np.array(S_t)\n",
    "     u_t[indices]=np.random.normal(size=len(indices))\n",
    "     return(u_t/np.linalg.norm(u_t))\n",
    "\n",
    "\n",
    "def psi(x,a=2):\n",
    "    return ((1/2*(a-1))*(np.linalg.norm(x, ord=a)**2))\n",
    "\n",
    "\n",
    "def Delta_psi(psi,x,y,a=2):\n",
    "    gradient_psi= nd.Gradient(psi)(x,a)\n",
    "    return(psi(y,a)-psi(x,a)-( gradient_psi.T@(y-x)))\n",
    "\n",
    "\n",
    "\n",
    "def constraint (x,B):\n",
    "    return (np.linalg.norm(x,ord=1) - B)\n",
    "\n",
    "\n",
    "def miror_descent_objectif_function(x,nu,g_t_tilde,x_t,psi,a=2):\n",
    "    return(nu*g_t_tilde.T*(x-x_t)+Delta_psi(psi=psi,x=x,y=x_t,a=a))\n",
    "\n",
    "\n",
    "def miror_descent_argmin(objectif_function,eta,g_t_tilde,x_t,constraint,psi,B,a=2):\n",
    "    d= x_t.shape[0]\n",
    "    x0 = np.random.normal(size=(d,))\n",
    "    result =  fmin_cobyla(objectif_function, x0, args=(eta,g_t_tilde,x_t,psi,a),  cons=constraint,consargs=(3,))\n",
    "    return(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradiantEstimate(x_t_vect:np.ndarray,d,n,delta,f,S):\n",
    "    \"\"\"\n",
    "    This function corresponds to the pseudo algorithme 1 defined in the paper\n",
    "    \"\"\"\n",
    "    Z=Rademacher_matrix(d,n)\n",
    "    y_t_vect=np.zeros(n)\n",
    "    for i in range(n):\n",
    "        z_i=Z[:, i]\n",
    "        y_t_vect[i] = f(x_t_vect+delta*z_i,S) #Construct the vector y_t:= f(x_t+delta*z_i)+noise. We suppose that f returns the true output plus noise\n",
    "    y_tilde=y_t_vect/delta\n",
    "    (g,u)=Lasso_reg(Y_tilde=y_tilde,Z=Z.T)\n",
    "    return(g,u)\n",
    "\n",
    "\n",
    "def debiased_Lasso(x_t_vect:np.ndarray,d,n,delta,f,S):\n",
    "     \"\"\"\n",
    "    This function returns the debiased lasso\n",
    "    \"\"\"\n",
    "\n",
    "     Z=Rademacher_matrix(d,n)\n",
    "     y_t_vect=np.zeros(n)\n",
    "     ones=np.ones(n)\n",
    "     for i in range(n):\n",
    "        z_i=Z[:, i]\n",
    "        y_t_vect[i] = f(x_t_vect+delta*z_i,S) #Construct the vector y_t:= f(x_t+delta*z_i)+noise. We suppose that f returns the true output plus noise\n",
    "     y_tilde=y_t_vect/delta\n",
    "     (g,u)=Lasso_reg(Y_tilde=y_tilde,Z=Z.T)\n",
    "     g_tilde=g+((1/n)*Z@(y_tilde-u*ones))\n",
    "     return(g_tilde)\n",
    "\n",
    "    \n",
    "def BGD(delta,T_prime,f,d,y_t,S_t,S,nu=1):\n",
    "        for t_prime in range (T_prime):\n",
    "             u_t=random_unit_vector_on_S_t(d=d,S_t=S_t) # select the unit vector u_t uniformly at random\n",
    "             x_t=y_t+delta*u_t #update x_t at each step where x_t=y_t+delta*u_t.\n",
    "             c_t=(f(x_t+delta*u_t,S)-f(x_t+delta*u_t,S))/(2*delta)\n",
    "             y_t=y_t-(nu*c_t*len(S_t)*u_t)\n",
    "        return x_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Successive_selection_algo(T,eta,delta,f,s,B,d,n,S,f_star=0,x_star=0,nu=1):\n",
    "    x_t_vect=np.zeros(d)\n",
    "    S_hat_t=[]\n",
    "    S_hat_t_minus_1=[1]\n",
    "    #chi_tilde={x for x in chi if np.linalg.norm(x,ord=1)<=B}\n",
    "    T_prime=int(T/2)\n",
    "    simple_regret=[]\n",
    "    cumulative_regret=[]\n",
    "    t=0\n",
    "    x_t_cost=[]\n",
    "    while len(S_hat_t)<s and t<s and S_hat_t!=S_hat_t_minus_1  : # setting  conditions to stop the while loop \n",
    "        \n",
    "        t=t+1\n",
    "        g_hat_t,u_t=GradiantEstimate(x_t_vect=x_t_vect,d=d,delta=delta,f=f,S=S,n=T_prime) # Estimate the gradient g_t \n",
    "        right_set=[i for i in range(d) if abs(g_hat_t[i]) >= eta]\n",
    "        S_hat_t_minus_1=S_hat_t.copy() # update S_hat_minus_1\n",
    "        S_hat_t=S_hat_t_minus_1 + (right_set) # Thresholding\n",
    "        S_hat_t=list(set(S_hat_t)) #To remove duplicates from S_hat_t\n",
    "        if len(S_hat_t)==0:\n",
    "             break\n",
    "        else:\n",
    "            x_t_minus_one=x_t_vect.copy() # Keep a copy of x_t to use it for the output \n",
    "            x_t_vect=BGD(T_prime=T_prime,f=f,d=d,y_t=x_t_vect,S_t=S_hat_t,delta=delta,S=S,nu=nu) # performe the finite difference algorithme that returns x_t\n",
    "            simple_regret.append(f(x_t_vect,S)-f_star)\n",
    "            cumulative_regret.append(((sum(cumulative_regret)+f(x_t_vect,S))/(len(cumulative_regret)+1))-f_star)\n",
    "            x_t_cost.append(np.linalg.norm(x_star-x_t_vect))\n",
    "        \n",
    "\n",
    "    if len(S_hat_t)==s:\n",
    "         return(x_t_vect,simple_regret,cumulative_regret,x_t_cost)\n",
    "    elif len(S_hat_t)==0: \n",
    "         print('In this case the algorithme did not make any discovery, it did not work')\n",
    "         return([],[],[],[])\n",
    "    else:\n",
    "         return(x_t_minus_one,simple_regret,cumulative_regret,x_t_cost)\n",
    "    \n",
    "\n",
    "def First_order_optimization(B,T,n,d,psi,eta,delta,S,f,objectif_function,constraint,f_star=0,x_star=0):\n",
    "     x_t_vect=np.zeros(d)\n",
    "     T_prime=int(T/2)\n",
    "     simple_regret=[]\n",
    "     cumulative_regret=[]\n",
    "     cost=[]\n",
    "     for t in range(T_prime):\n",
    "          g_t_tide=debiased_Lasso(x_t_vect=x_t_vect,d=d,n=n,delta=delta,f=f,S=S)\n",
    "          x_t_vect=miror_descent_argmin(objectif_function=objectif_function,eta=eta,g_t_tilde=g_t_tide,x_t=x_t_vect,psi=psi,B=B,a=2,constraint=constraint)\n",
    "          simple_regret.append(f(x_t_vect,S)-f_star)\n",
    "          cumulative_regret.append(((sum(cumulative_regret)+f(x_t_vect,S))/(len(cumulative_regret)+1))-f_star)\n",
    "          cost.append(np.linalg.norm(x_star-x_t_vect))\n",
    "     return(x_t_vect,simple_regret,cumulative_regret,cost)\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that will be used for testing \n",
    "def f_test(x_t,noise=0):#S le support \n",
    "    return(np.linalg.norm(x_t)**2+noise*np.random.normal(0,1,1))\n",
    "def vect_f_test(x_t, delta, d, n):\n",
    "    y_t_vecteur = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        y_t_vecteur[i] = f_test(x_t, delta, d)\n",
    "    y_t = f_test(x_t=x_t, delta=0, d=d, noise=0)\n",
    "    return y_t_vecteur, y_t\n",
    "def f_test_S(X_t,S,noise=1):\n",
    "    X_S=np.array([X_t[i] if i in S else 0 for i in range(len(X_t))])\n",
    "    return(np.linalg.norm(X_S)**2+np.sum(X_S))+noise*np.random.normal(0,1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that will be used to estimate hyperparameters\n",
    "\n",
    "def logarithmic_grid_delta(sigma,s,T,d,H,nb_pnt):\n",
    "    delta=(((sigma**2)*s*d)/(T*(H**2)))**(1/4)\n",
    "    begin=delta/nb_pnt\n",
    "    stop=delta*nb_pnt\n",
    "    return(np.logspace(np.log10(begin), np.log10(stop), num=nb_pnt))\n",
    "\n",
    "\n",
    "def logarithmic_grid_eta(C,d,n,nb_pnt):\n",
    "    eta=(log(d)/n)**(1/2)\n",
    "    begin=eta/nb_pnt\n",
    "    stop=eta*nb_pnt\n",
    "    return(np.logspace(np.log10(begin), np.log10(stop), num=nb_pnt))\n",
    "\n",
    "\n",
    "def logarithmic_grid_nu(n,nb_pnt):\n",
    "    nu=1/n\n",
    "    begin= nu/nb_pnt\n",
    "    stop= nu/nb_pnt\n",
    "    return(np.logspace(np.log10(begin), np.log10(stop), num=nb_pnt))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precising the parameters \n",
    "d=100\n",
    "n=50\n",
    "delta=2\n",
    "eta=2\n",
    "\n",
    "x_t=np.random.binomial(1, 1/2,size=(d,))\n",
    "#x_star = random.choices([1/2], k=d)\n",
    "s=20\n",
    "S= random.sample(range(90), s)\n",
    "T=30\n",
    "B=2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star=np.zeros(d)\n",
    "indices=np.array(S)\n",
    "x_star[indices]=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=1\n",
    "H=-s\n",
    "nb_pnt=10\n",
    "C=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logarithmic_grid_eta=logarithmic_grid_eta(C=C,d=d,n=n,nb_pnt=nb_pnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logarithmic_grid_delta=logarithmic_grid_delta(sigma=sigma,s=s,T=T,d=d,H=H,nb_pnt=nb_pnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logarithmic_grid_nu=logarithmic_grid_nu(n=n,nb_pnt=nb_pnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_star=f_test_S(x_star,S,noise=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t,simple_regret,cumulative_regret,x_cost=Successive_selection_algo(T=T,eta=eta,delta=delta,f=f_test_S,s=s,B=B,d=d,n=n,f_star=f_star,S=S,x_star=x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5), dpi=100)\n",
    "plt.plot(list(range(1,len(cumulative_regret)+1)),np.log(cumulative_regret))\n",
    "plt.title(\"Représentation du regret cumulatif \")  # Titre du graphique\n",
    "plt.ylabel('log(regret cumulatif)')  # Titre de l'axe y\n",
    "plt.xlabel('nombre itérations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5), dpi=100)\n",
    "plt.plot(list(range(1,len(simple_regret)+1)),np.log(simple_regret))\n",
    "plt.title(\"Représentation du regret simple \")  # Titre du graphique\n",
    "plt.ylabel('log(regret simple)')  # Titre de l'axe y\n",
    "plt.xlabel('nombre itérations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5), dpi=100)\n",
    "plt.plot(list(range(1,len(x_cost)+1)),np.log(x_cost))\n",
    "plt.title(\"|| x_star - x_t|| \")  # Titre du graphique\n",
    "plt.ylabel('|| x_star - x_t||')  # Titre de l'axe y\n",
    "plt.xlabel('nombre itérations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for delta in logarithmic_grid_delta:\n",
    "    for eta in logarithmic_grid_eta:\n",
    "        for nu in logarithmic_grid_nu:\n",
    "            x_t,simple_regret,cumulative_regret,x_cost=Successive_selection_algo(T=T,eta=eta,delta=delta,f=f_test_S,s=s,B=B,d=d,n=n,f_star=f_star,S=S,x_star=x_star,nu=nu)\n",
    "            if len(simple_regret)!=0 and len(simple_regret)!=1 :\n",
    "                plt.figure(figsize=(10, 5), dpi=100)\n",
    "                plt.title(f\"Résultats pour delta:{delta} et eta:{eta} et nu:{nu}\")  # Titre du graphique\n",
    "                plt.ylabel('cost')  # Titre de l'axe y\n",
    "                plt.xlabel('nombre itérations')\n",
    "                plt.plot(list(range(1,len(simple_regret)+1)),np.log(list(map(abs, simple_regret))),color='blue',label='regret simple')\n",
    "                plt.plot(list(range(1,len(cumulative_regret)+1)),np.log(list(map(abs, cumulative_regret))),color='red',label='regret cumulatif')\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t,simple_regret,cumulative_regret,x_cost=First_order_optimization(B=B,T=T,n=n,d=d,psi=psi,eta=eta,delta=delta,S=S,f=f_test_S,objectif_function=miror_descent_objectif_function,f_star=f_star,x_star=x_star,constraint=constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
