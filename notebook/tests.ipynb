{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import random\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define funcitons that will be useful for algorithms simulations \n",
    "\n",
    "def Rademacher_matrix(d,n):\n",
    "    \"\"\"\n",
    "    This fucntion generates a Rademacher matrix\n",
    "    \"\"\"\n",
    "    return np.random.choice([-1, 1], size=(d,n))\n",
    "\n",
    "\n",
    "\n",
    "def Rademacher_matrix_concatenated(d,n):\n",
    "    \"\"\"\n",
    "    This function generates a Rademacher matrix and add a line of ones\n",
    "    \"\"\"\n",
    "    Z=Rademacher_matrix(d,n)\n",
    "    Last_line_of_ones = np.ones((1, Z.shape[1]))\n",
    "    return (Z,np.concatenate((Z, Last_line_of_ones), axis=0))\n",
    "\n",
    "\n",
    "\n",
    "def Lasso_reg(Y_tilde,Z):\n",
    "    \"\"\"\n",
    "    This function gives the solution to the Lasso regression in a multivariate model\n",
    "    \"\"\"\n",
    "    lasso = linear_model.LassoCV(cv=5)\n",
    "    lasso.fit(Z,Y_tilde )\n",
    "    g = lasso.coef_\n",
    "    u=lasso.intercept_\n",
    "    return(g,u)\n",
    "\n",
    "\n",
    "\n",
    "def random_vector_unit_spehre(d):\n",
    "    random_vector = np.random.normal(size=d)\n",
    "    return(random_vector/np.linalg.norm(random_vector))\n",
    "\n",
    "\n",
    "\n",
    "def random_unit_vector_on_S_t(d,S_t):\n",
    "     \"\"\"\n",
    "     This function simulates a unit random vector on the spehre S_t\n",
    "     \"\"\"\n",
    "     u_t=np.zeros(d)\n",
    "     indices=np.array(S_t)\n",
    "     u_t[indices]=np.random.normal(size=len(indices))\n",
    "     return(u_t/np.linalg.norm(u_t))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradiantEstimate(x_t_vect:np.ndarray,d,n,delta,f,S):\n",
    "    \"\"\"\n",
    "    This function corresponds to the pseudo algorithme 1 defined in the paper\n",
    "    \"\"\"\n",
    "    Z=Rademacher_matrix(d,n)\n",
    "    y_t_vect=np.zeros(n)\n",
    "    for i in range(n):\n",
    "        z_i=Z[:, i]\n",
    "        y_t_vect[i] = f(x_t_vect+delta*z_i,S) #Construct the vector y_t:= f(x_t+delta*z_i)+noise. We suppose that f returns the true output plus noise\n",
    "    y_tilde=y_t_vect/delta\n",
    "    (g,u)=Lasso_reg(Y_tilde=y_tilde,Z=Z.T)\n",
    "    return(g,u)\n",
    "\n",
    "\n",
    "    \n",
    "def BGD(delta,T_prime,f,d,y_t,S_t,S,nu=1):\n",
    "        for t_prime in range (T_prime):\n",
    "             u_t=random_unit_vector_on_S_t(d=d,S_t=S_t) # select the unit vector u_t uniformly at random\n",
    "             x_t=y_t+delta*u_t #update x_t at each step where x_t=y_t+delta*u_t.\n",
    "             c_t=(f(x_t+delta*u_t,S)-f(x_t+delta*u_t,S))/(2*delta)\n",
    "             y_t=y_t-(nu*c_t*len(S_t)*u_t)\n",
    "        return x_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Successive_selection_algo(T,eta,delta,f,s,B,d,n,S,f_star=0):\n",
    "    x_t_vect=np.zeros(d)\n",
    "    S_hat_t=[]\n",
    "    S_hat_t_minus_1=[1]\n",
    "    #chi_tilde={x for x in chi if np.linalg.norm(x,ord=1)<=B}\n",
    "    T_prime=int(T/2)\n",
    "    simple_regret=[]\n",
    "    cumulative_regret=[]\n",
    "    t=0\n",
    "    while len(S_hat_t)<s and t<s and S_hat_t!=S_hat_t_minus_1  : # setting  conditions to stop the while loop \n",
    "        \n",
    "        S_hat_t_copy=S_hat_t.copy() # creating a copy of S_hat_t that will be used to update S_hat_t_minus_1\n",
    "        t=t+1\n",
    "        g_hat_t,u_t=GradiantEstimate(x_t_vect,d,n,delta,f,S) # Estimate the gradient g_t \n",
    "        right_set=[i for i in range(d) if abs(g_hat_t[i]) >= eta]\n",
    "        S_hat_t=S_hat_t_minus_1 + (right_set) # Thresholding\n",
    "        S_hat_t_minus_1=S_hat_t_copy # update S_hat_minus_1\n",
    "        x_t_minus_one=x_t_vect.copy() # Keep a copy of x_t to use it for the output \n",
    "        x_t_vect=BGD(T_prime=T_prime,f=f,d=d,y_t=x_t_vect,S_t=S_hat_t,delta=delta,S=S) # performe the finite difference algorithme that returns x_t\n",
    "        simple_regret.append(f(x_t_vect,S)-f_star)\n",
    "        cumulative_regret.append(((sum(cumulative_regret)+f(x_t_vect,S))/(len(cumulative_regret)+1))-f_star)\n",
    "\n",
    "    if len(S_hat_t)==s:\n",
    "         return(x_t_vect,simple_regret,cumulative_regret)\n",
    "    else:\n",
    "         return(x_t_minus_one,simple_regret,cumulative_regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that will be used for testing \n",
    "def f_test(x_t,noise=1):#S le support \n",
    "    return(np.linalg.norm(x_t)+noise*np.random.normal(0,1,1))\n",
    "def vect_f_test(x_t, delta, d, n):\n",
    "    y_t_vecteur = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        y_t_vecteur[i] = f_test(x_t, delta, d)\n",
    "    y_t = f_test(x_t=x_t, delta=0, d=d, noise=0)\n",
    "    return y_t_vecteur, y_t\n",
    "def f_test_S(X_t,S,noise=1):\n",
    "    X_S=np.array([X_t[i] if i in S else 0 for i in range(len(X_t))])\n",
    "    return(np.linalg.norm(X_S)**2+np.sum(X_S))+noise*np.random.normal(0,1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that will be used to estimate hyperparameters\n",
    "\n",
    "def logarithmic_grid_delta(sigma,s,T,d,H,nb_pnt):\n",
    "    delta=(((sigma**2)*s*d)/(T*(H**2)))**(1/4)\n",
    "    begin=delta/nb_pnt\n",
    "    stop=delta*nb_pnt\n",
    "    return(np.logspace(np.log10(begin), np.log10(stop), num=nb_pnt))\n",
    "\n",
    "\n",
    "def logarithmic_grid_eta(C,d,n,nb_pnt):\n",
    "    eta=(log(d)/n)**(1/2)\n",
    "    begin=eta/nb_pnt\n",
    "    stop=eta*nb_pnt\n",
    "    return(np.logspace(np.log10(begin), np.log10(stop), num=nb_pnt))\n",
    "\n",
    "\n",
    "def logarithmic_grid_nu(n,nb_pnt):\n",
    "    nu=1/n\n",
    "    begin= nu/nb_pnt\n",
    "    stop= nu/nb_pnt\n",
    "    return(np.logspace(np.log10(begin), np.log10(stop), num=nb_pnt))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precising the parameters \n",
    "d=100\n",
    "n=50\n",
    "delta=0.5\n",
    "lamda=0.1\n",
    "x_t=np.random.binomial(1, 1/2,size=(d,))\n",
    "S= random.sample(range(90), 20)\n",
    "x_star = random.choices([1/2], k=d)\n",
    "f_star=f_test_S(x_star,S,noise=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t,simple_regret,cumulative_regret=Successive_selection_algo(T=100,eta=0,delta=2,f=f_test_S,s=10,B=2,d=d,n=n,f_star=f_star,S=S)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
