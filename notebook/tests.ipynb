{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install  numdifftools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import random\n",
    "from math import log,sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import numdifftools as nd\n",
    "import warnings\n",
    "from scipy.optimize import minimize\n",
    "import torch\n",
    "from torch import optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define funcitons that will be useful for algorithms simulations \n",
    "\n",
    "def Rademacher_matrix(d,n):\n",
    "    \"\"\"\n",
    "    This fucntion generates a Rademacher matrix\n",
    "    \"\"\"\n",
    "    return np.random.choice([-1, 1], size=(d,n))\n",
    "\n",
    "\n",
    "\n",
    "def Rademacher_matrix_concatenated(d,n):\n",
    "    \"\"\"\n",
    "    This function generates a Rademacher matrix and add a line of ones\n",
    "    \"\"\"\n",
    "    Z=Rademacher_matrix(d,n)\n",
    "    Last_line_of_ones = np.ones((1, Z.shape[1]))\n",
    "    return (Z,np.concatenate((Z, Last_line_of_ones), axis=0))\n",
    "\n",
    "\n",
    "\n",
    "def Lasso_reg(Y_tilde,Z):\n",
    "    \"\"\"\n",
    "    This function gives the solution to the Lasso regression in a multivariate model\n",
    "    \"\"\"\n",
    "    lasso = linear_model.LassoCV(cv=5)\n",
    "    lasso.fit(Z,Y_tilde )\n",
    "    g = lasso.coef_\n",
    "    u=lasso.intercept_\n",
    "    return(g,u)\n",
    "\n",
    "\n",
    "\n",
    "def random_vector_unit_spehre(d):\n",
    "    random_vector = np.random.normal(size=d)\n",
    "    return(random_vector/np.linalg.norm(random_vector))\n",
    "\n",
    "\n",
    "\n",
    "def random_unit_vector_on_S_t(d,S_t):\n",
    "     \"\"\"\n",
    "     This function simulates a unit random vector on the spehre S_t\n",
    "     \"\"\"\n",
    "     u_t=np.zeros(d)\n",
    "     indices=np.array(S_t)\n",
    "     u_t[indices]=np.random.normal(size=len(indices))\n",
    "     return(u_t/np.linalg.norm(u_t))\n",
    "\n",
    "\n",
    "def psi(x,a=2):\n",
    "    \"\"\" \n",
    "    This fonction computes the operator psi(x)= 1/2(a-1)*||x||_{a}^{2}. The choice of a must be such that: 1<a<=2. \n",
    "    \"\"\"\n",
    "    return ((1/2*(a-1))*(np.linalg.norm(x, ord=a)**2))\n",
    "\n",
    "\n",
    "def Delta_psi(psi,x,y,a=2):\n",
    "    \"\"\" \n",
    "    This function computes the operator Delta_psi(x,y)= psi(y)-psi(x)-<nabla(psi(x)),y-x>.\n",
    "    This approach is the most general one, since a numerical methode is used to compute the gradient of the function psi.\n",
    "    \"\"\"\n",
    "    gradient_psi= nd.Gradient(psi)(x,a)\n",
    "    return(psi(y,a)-psi(x,a)-( gradient_psi.T@(y-x)))\n",
    "\n",
    "\n",
    "\n",
    "def constraint (x,B):\n",
    "    \"\"\" \n",
    "    This function defines the constraint that x is within the sphere of radius B . \n",
    "    \"\"\"\n",
    "    return (np.linalg.norm(x,ord=1) - B)\n",
    "\n",
    "\n",
    "def miror_descent_objectif_function_general(x,nu,g_t_tilde,x_t,psi,a=2):\n",
    "    \"\"\" \n",
    "    This is the function to optimise fro the MD update step. The following function corresponds to the general case. \n",
    "\n",
    "    \"\"\"\n",
    "    result=nu*g_t_tilde.T@(x-x_t)+Delta_psi(psi=psi,x=x,y=x_t,a=a)\n",
    "    return(result[0])\n",
    "\n",
    "\n",
    "def miror_descent_objectif_function_a_2(x,nu,g_t_tilde,x_t):\n",
    "    \"\"\" \n",
    "    This is the function to optimise fro the MD update step. It corresponds to the particular case where a is equal to 2. \n",
    "    \"\"\"\n",
    "    result=nu*g_t_tilde.T@(x-x_t)+1/2*(np.linalg.norm(x-x_t)**2)\n",
    "    return(result[0])\n",
    "\n",
    "\n",
    "\n",
    "def miror_descent_argmin(objectif_function,eta,g_t_tilde,x_t,constraint,psi,B,a=2):\n",
    "    \"\"\"\n",
    "    This function computes the minimum of the function to optimize during the MD update step. \n",
    "    \"\"\"\n",
    "    d= x_t.shape[0]\n",
    "    x0 = np.random.normal(size=(d,))\n",
    "    constraints = ({'type': 'ineq', 'fun': lambda x: constraint(x, B)})\n",
    "    result=minimize(objectif_function, x0,args=(eta,g_t_tilde,x_t,psi,a) ,constraints=constraints)\n",
    "    return(result.x)\n",
    "\n",
    "\n",
    "def optimiser_torch(f,d,nb_it=100,learning_rate=0.1,*args, **kwargs):\n",
    "     \"\"\" \n",
    "     This function computes the minimum of a given function using torch methods\n",
    "     \"\"\"\n",
    "     X_t = torch.ones(d, requires_grad=True) # Fix an intial guess. Here we fix the intial guess to be a vector of ones \n",
    "     optimizer = torch.optim.Adam([X_t], lr=learning_rate)\n",
    "     for i in range(nb_it):\n",
    "        optimizer.zero_grad()\n",
    "        loss_value = f(X_t,*args, **kwargs)\n",
    "        loss = torch.tensor(loss_value, dtype=torch.float32, requires_grad=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "     return X_t\n",
    "\n",
    "\n",
    "def optimiser_scipy(f,d,S,noise=0,methode='trust-constr'):\n",
    "    \"\"\" \n",
    "    This function computes the minimum of a given function using scipy methods. \n",
    "    to chose the methode you can check this link : https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "    \"\"\"\n",
    "    initial_guess = np.zeros(d)# Fix an intial guess. Here we fix the intial guess to be a vector of ones\n",
    "    min = minimize(f, initial_guess,args=(S,noise),method=methode)\n",
    "    return( min.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is for testing a methode to solve the MD \n",
    "def calculate_ratio(x):\n",
    "    result = np.zeros_like(x)  # Create an array of zeros with the same shape as x\n",
    "\n",
    "    for i, val in enumerate(x):\n",
    "        if val != 0:\n",
    "            result[i] = val / abs(val)\n",
    "        else:\n",
    "            result[i] = 0  \n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def objective_lagragien(x,x_t,g_t_tilde,eta,nu,B):\n",
    "    d=x_t.shape[0]\n",
    "    x_t=x_t.reshape((d, 1))\n",
    "    x=x.reshape((d, 1))\n",
    "    x_lamda=calculate_ratio(x)\n",
    "    lamda=(x_lamda.T)@(eta*g_t_tilde+(x-x_t))/d\n",
    "    result=miror_descent_objectif_function_a_2(x=x,nu=nu,g_t_tilde=g_t_tilde,x_t=x_t)+lamda*(np.linalg.norm(x, ord=1))-B\n",
    "    return(result[0][0])\n",
    "\n",
    "\n",
    "def opt_MD(f,d,methode='trust-constr',*args):\n",
    "    initial_guess = np.zeros(d)# Fix an intial guess. Here we fix the intial guess to be a vector of ones\n",
    "    min = minimize(f, initial_guess,args=args,method=methode)\n",
    "    return( min.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradiantEstimate(x_t_vect:np.ndarray,d,n,delta,f,S):\n",
    "    \"\"\"\n",
    "    This function corresponds to the pseudo algorithme 1 defined in the paper\n",
    "    \"\"\"\n",
    "    Z=Rademacher_matrix(d,n)\n",
    "    y_t_vect=np.zeros(n)\n",
    "    for i in range(n):\n",
    "        z_i=Z[:, i]\n",
    "        y_t_vect[i] = f(x_t_vect+delta*z_i,S) #Construct the vector y_t:= f(x_t+delta*z_i)+noise. We suppose that f returns the true output plus noise\n",
    "    y_tilde=y_t_vect/delta\n",
    "    (g,u)=Lasso_reg(Y_tilde=y_tilde,Z=Z.T)\n",
    "    return(g,u)\n",
    "\n",
    "\n",
    "def debiased_Lasso(x_t_vect:np.ndarray,d,n,delta,f,S):\n",
    "     \"\"\"\n",
    "    This function computes  the debiased lasso\n",
    "    \"\"\"\n",
    "\n",
    "     Z=Rademacher_matrix(d,n)\n",
    "     y_t_vect=np.zeros(n)\n",
    "     ones=np.ones(n)\n",
    "     for i in range(n):\n",
    "        z_i=Z[:, i]\n",
    "        y_t_vect[i] = f(x_t_vect+delta*z_i,S) #Construct the vector y_t:= f(x_t+delta*z_i)+noise. We suppose that f returns the true output plus noise\n",
    "     y_tilde=y_t_vect/delta\n",
    "     (g,u)=Lasso_reg(Y_tilde=y_tilde,Z=Z.T)\n",
    "     g_tilde=g+((1/n)*Z@(y_tilde-u*ones))\n",
    "     d_s=g_tilde.shape[0]\n",
    "     g_tilde=g_tilde.reshape((d_s, 1))\n",
    "     return(g_tilde)\n",
    "\n",
    "    \n",
    "def BGD(delta,T_prime,f,d,y_t,S_t,S,nu=1):\n",
    "        for t_prime in range (T_prime):\n",
    "             u_t=random_unit_vector_on_S_t(d=d,S_t=S_t) # select the unit vector u_t uniformly at random\n",
    "             x_t=y_t+delta*u_t #update x_t at each step where x_t=y_t+delta*u_t.\n",
    "             c_t=(f(x_t+delta*u_t,S)-f(x_t+delta*u_t,S))/(2*delta)\n",
    "             y_t=y_t-(nu*c_t*len(S_t)*u_t)\n",
    "        return x_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Successive_selection_algo(T,eta,delta,f,s,d,S,f_star=0,x_star=0,nu=1):\n",
    "    \"\"\" \n",
    "    This function computes the Successive selection algorithm. \n",
    "    The function will return empty lists if it's impossible to compute the algorithme\n",
    "    Parameters:\n",
    "    ----------\n",
    "    T: is the budget.\n",
    "    Eta: the threshold value\n",
    "    Delta:A parameter to compute the neighborhood of the f(x_t)\n",
    "    s: is the sparsity level\n",
    "    f: is the black-box function to optimize\n",
    "    f_star is the true minimum of f. It is used in this function just for testing perspectives.\n",
    "    x_star is the true argmin of f. It is used in this function just for testing perspectives.\n",
    "\n",
    "    \"\"\"\n",
    "    x_t_vect=np.zeros(d)\n",
    "    S_hat_t=[]\n",
    "    S_hat_t_minus_1=[1]\n",
    "    #chi_tilde={x for x in chi if np.linalg.norm(x,ord=1)<=B}\n",
    "    T_prime=int(T/2)\n",
    "    simple_regret=[]\n",
    "    cumulative_regret=[]\n",
    "    t=0\n",
    "    x_t_cost=[]\n",
    "    while   t<s and S_hat_t!=S_hat_t_minus_1  : # setting  conditions to stop the while loop  len(S_hat_t)<s\n",
    "        \n",
    "        t=t+1\n",
    "        g_hat_t,u_t=GradiantEstimate(x_t_vect=x_t_vect,d=d,delta=delta,f=f,S=S,n=T_prime) # Estimate the gradient g_t \n",
    "        right_set=[i for i in range(d) if abs(g_hat_t[i]) >= eta]\n",
    "        S_hat_t_minus_1=S_hat_t.copy() # update S_hat_minus_1\n",
    "        S_hat_t=S_hat_t_minus_1 + (right_set) # Thresholding\n",
    "        S_hat_t=list(set(S_hat_t)) #To remove duplicates from S_hat_t\n",
    "        if len(S_hat_t)==0:\n",
    "             break\n",
    "        else:\n",
    "            x_t_minus_one=x_t_vect.copy() # Keep a copy of x_t to use it for the output \n",
    "            x_t_vect=BGD(T_prime=T_prime,f=f,d=d,y_t=x_t_vect,S_t=S_hat_t,delta=delta,S=S,nu=nu) # performe the finite difference algorithme that returns x_t\n",
    "            simple_regret.append(f(x_t_vect,S,noise=0)-f_star)\n",
    "            cumulative_regret.append(((sum(cumulative_regret)+f(x_t_vect,S,noise=0))/(len(cumulative_regret)+1))-f_star)\n",
    "            x_t_cost.append(np.linalg.norm(x_star-x_t_vect))\n",
    "        \n",
    "\n",
    "    if len(S_hat_t)==s:\n",
    "         return(x_t_vect,simple_regret,cumulative_regret,x_t_cost)\n",
    "    elif len(S_hat_t)==0: \n",
    "         warnings.warn(\"Algorithm did not make any discovery\",stacklevel=2)\n",
    "         return([],[],[],[])\n",
    "\n",
    "         \n",
    "    else:\n",
    "         return(x_t_minus_one,simple_regret,cumulative_regret,x_t_cost)\n",
    "    \n",
    "\n",
    "\n",
    "def First_order_optimization(B,T,n,d,psi,eta,delta,S,f,objectif_function,constraint,f_star=0,x_star=0):\n",
    "     \"\"\"\n",
    "    This function computes the First-order mirror descent with estimated gradients. \n",
    "    The function will return empty lists if it is suspected that the algorithm will diverge. \n",
    "    You should then change the hyperparameters.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    T: is the budget.\n",
    "    Eta: the hyperpameter for the MD update\n",
    "    Delta:A parameter to compute the neighborhood of the f(x_t)\n",
    "    s: is the sparsity level\n",
    "    f: is the black-box function to optimize\n",
    "    f_star is the true minimum of f. It is used in this function just for testing perspectives.\n",
    "    x_star is the true argmin of f. It is used in this function just for testing perspectives.\n",
    "    Constraint: is such that x belongs to the sphere of radius B. \n",
    "    This will be removed when this code is put into a class.\n",
    "\n",
    "     \"\"\"\n",
    "     x_t_vect=np.zeros(d)\n",
    "     T_prime=int(T/2)\n",
    "     simple_regret=[]\n",
    "     cumulative_regret=[]\n",
    "     cost=[]\n",
    "     for t in range(T_prime):\n",
    "          g_t_tide=debiased_Lasso(x_t_vect=x_t_vect,d=d,n=n,delta=delta,f=f,S=S)\n",
    "          x_t_vect=miror_descent_argmin(objectif_function=objectif_function,eta=eta,g_t_tilde=g_t_tide,x_t=x_t_vect,psi=psi,B=B,a=2,constraint=constraint)\n",
    "          simple_regret.append(f(x_t_vect,S,noise=0)-f_star)\n",
    "          cumulative_regret.append(((sum(cumulative_regret)+f(x_t_vect,S,noise=0))/(len(cumulative_regret)+1))-f_star)\n",
    "          cost.append(np.linalg.norm(x_star-x_t_vect)) \n",
    "          if len(cost)>1:\n",
    "           if cost[-1]-cost[-2]>1e3:\n",
    "               warnings.warn(\"The algorithm has started diverging, and the computation will be stopped.\",stacklevel=2)\n",
    "               break\n",
    "     return(x_t_vect,simple_regret,cumulative_regret,cost)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that will be used for testing \n",
    "def f_test(x_t,noise=0):#S le support \n",
    "    return(np.linalg.norm(x_t)**2+noise*np.random.normal(0,1,1))\n",
    "\n",
    "\n",
    "\n",
    "def vect_f_test(x_t, delta, d, n):\n",
    "    y_t_vecteur = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        y_t_vecteur[i] = f_test(x_t, delta, d)\n",
    "    y_t = f_test(x_t=x_t, delta=0, d=d, noise=0)\n",
    "    return y_t_vecteur, y_t\n",
    "\n",
    "\n",
    "\n",
    "def f_test_S(X_t,S,noise=1):\n",
    "    \"\"\" \n",
    "    This function computes : f_{S}(x)=sum_{i in S}(x_{i}^{2}+ x_{i})+ sigma*zeta , where zeta is a random noise. \n",
    "    Here the noise is choosen to be a random normal variable \n",
    "    \n",
    "    \"\"\"\n",
    "    X_S=np.array([X_t[i] if i in S else 0 for i in range(len(X_t))])\n",
    "    return(np.linalg.norm(X_S)**2+np.sum(X_S))+noise*np.random.normal(0,1,1)\n",
    "\n",
    "\n",
    "\n",
    "def f_test_S_torch(X_t, S, noise=1): \n",
    "    \"\"\" \n",
    "    This is the same as the function bellow just using torch methods\n",
    "    \"\"\"\n",
    "    X_S = torch.tensor([X_t[i] if i in S else 0 for i in range(len(X_t))], dtype=torch.float32)\n",
    "    return ((torch.norm(X_S)**2 + torch.sum(X_S) + noise * torch.normal(0, 1, size=(1,))).item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that will be used to estimate hyperparameters for the successive component selection algorithm: logarithmic grids\n",
    "\n",
    "def logarithmic_grid_delta(sigma,s,T,d,H,nb_pnt):\n",
    "    delta=(((sigma**2)*s*d)/(T*(H**2)))**(1/4)\n",
    "    begin=delta/nb_pnt\n",
    "    stop=delta*nb_pnt\n",
    "    return(np.logspace(np.log10(begin), np.log10(stop), num=nb_pnt))\n",
    "\n",
    "\n",
    "def logarithmic_grid_eta(C,d,n,nb_pnt):\n",
    "    eta=(log(d)/n)**(1/2)\n",
    "    begin=eta/nb_pnt\n",
    "    stop=eta*nb_pnt\n",
    "    return(np.logspace(np.log10(begin), np.log10(stop), num=nb_pnt))\n",
    "\n",
    "\n",
    "def logarithmic_grid_nu(n,nb_pnt):\n",
    "    nu=1/n\n",
    "    begin= nu/nb_pnt\n",
    "    stop= nu/nb_pnt\n",
    "    return(np.logspace(np.log10(begin), np.log10(stop), num=nb_pnt))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that will be used to estimate hyperparameters for the first-order miror descent with estimated gradients algorithme\n",
    "# This hyperparameters are equal to what it was suggested in Theoreme 2 of the article\n",
    "def T_miror_descent(s,d,H,B):\n",
    "    return(s**3)*(log(d)**2)+(s*(1+H)**2)*(1+((B*H)**4)*(log(d)**2))\n",
    "\n",
    "\n",
    "def n_miror_descent(T,s,H):\n",
    "    return(int((1+H)*sqrt(s*T)))\n",
    "\n",
    "\n",
    "def eta_miror_descent(B,d,T,n):\n",
    "   return( B*(sqrt((n*log(d))/T)))\n",
    "\n",
    "\n",
    "def delta_miror_descent(s,d,n):\n",
    "    return(sqrt(s*log(d/n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that will be used to estimate hyperparameters for the First-order mirror descent with estimated gradients: logarithmic grids\n",
    "def log_grid_eta_miror_descent(B,d,T,n,nb_pnt):\n",
    "   eta= B*(sqrt((n*log(d))/T))\n",
    "   begin=eta/nb_pnt\n",
    "   stop=eta*nb_pnt\n",
    "   return(np.logspace(np.log10(begin), np.log10(stop), num=nb_pnt))\n",
    "\n",
    "def log_grid_delta_miror_descent(s,d,n,nb_pnt):\n",
    "    beta=sqrt(s*log(d/n))\n",
    "    begin=beta/nb_pnt\n",
    "    stop=beta*nb_pnt\n",
    "    return(np.logspace(np.log10(begin), np.log10(stop), num=nb_pnt))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precising the parameters \n",
    "d=1000 # d is the dimension of the starting set of f \n",
    "n=100 #  n is a hyperparameter used to compute the neighborhood of f(x_t) in order to estimate the gradient\n",
    "\n",
    "\n",
    "x_t=np.random.binomial(1, 1/2,size=(d,1)) # a rondom vector to debug my functions \n",
    "s=20 # The sparsity level\n",
    "S= random.sample(range(90), s) # The set S fuch that f_{S}(x_{S})= f(x)\n",
    "T=10 # The budget\n",
    "\n",
    "\n",
    "\n",
    "sigma=1 # The noise level \n",
    "nb_pnt=10 # The number of points for logarithgmic grids. \n",
    "C=5 # A constant that will be used for estimating the hyperparameters \n",
    "x_star=optimiser_scipy(f_test_S,d=d,S=S,noise=0) # The argmin of the function\n",
    "f_star=f_test_S(x_star,S,noise=0) # The minimum of the function\n",
    "B= (np.linalg.norm(x_star))+1 # B is the Minimizer of bounded l1 -norm , ie ||x^{*}||_{1}< B\n",
    "H=B*s # H is such that || gradient(f(x))||_{1} <H\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_t=np.random.binomial(1, 1/2,size=(d,1))\n",
    "x=x_t=np.random.binomial(1, 1/2,size=(d,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_lagragien(x,x_t,g_t,eta,1,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_MD(objective_lagragien,d,x_t,g_t,eta,1,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the grids for the hyperpameters of the  successive component selection algorithme\n",
    "\n",
    "log_grid_eta=logarithmic_grid_eta(C=C,d=d,n=n,nb_pnt=nb_pnt)\n",
    "log_grid_delta=logarithmic_grid_delta(sigma=sigma,s=s,T=T,d=d,H=H,nb_pnt=nb_pnt)\n",
    "log_grid_nu=logarithmic_grid_nu(n=n,nb_pnt=nb_pnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for delta in log_grid_delta:\n",
    "    for eta in log_grid_eta:\n",
    "        for nu in log_grid_nu:\n",
    "            x_t,simple_regret,cumulative_regret,x_cost=Successive_selection_algo(T=T,eta=eta,delta=delta,f=f_test_S,s=s,d=d,f_star=f_star,S=S,x_star=x_star,nu=nu)\n",
    "            if len(simple_regret)!=0 and len(simple_regret)!=1 :\n",
    "                plt.figure(figsize=(10, 5), dpi=100)\n",
    "                plt.title(f\"Résultats pour delta:{delta} et eta:{eta} et nu:{nu}\")  # Titre du graphique\n",
    "                plt.ylabel('cost')  # Titre de l'axe y\n",
    "                plt.xlabel('nombre itérations')\n",
    "                plt.plot(list(range(1,len(simple_regret)+1)),np.log(list(map(abs, simple_regret))),color='blue',label='regret simple')\n",
    "                plt.plot(list(range(1,len(x_cost)+1)), x_cost,color='red',label='||x_star-x||')\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the grids for the hyperpameters of the  successive component selection algorithme\n",
    "logarithmic_grid_eta_MD=log_grid_eta_miror_descent(B=B,d=d,T=T,n=n,nb_pnt=nb_pnt)\n",
    "logarithmic_grid_dleta_MD=log_grid_delta_miror_descent(s=s,d=d,n=n,nb_pnt=nb_pnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for delta in logarithmic_grid_dleta_MD:\n",
    "    for eta in logarithmic_grid_eta_MD:\n",
    "        x_t,simple_regret,cumulative_regret,x_cost,=First_order_optimization(B=B,T=T,n=n,d=d,psi=psi,eta=eta,delta=delta,S=S,f=f_test_S,objectif_function=miror_descent_objectif_function_a_2,f_star=f_star,x_star=x_star,constraint=constraint)\n",
    "        plt.figure(figsize=(10, 5), dpi=100)\n",
    "        plt.title(f\"Résultats pour delta:{delta} et eta:{eta} \")  # Titre du graphique\n",
    "        plt.ylabel('cost')  # Titre de l'axe y\n",
    "        plt.xlabel('nombre itérations')\n",
    "        plt.plot(list(range(1,len(x_cost)+1)),np.log(list(map(abs, x_cost))),color='blue',label='||x_star-x||')\n",
    "        plt.plot(list(range(1,len(cumulative_regret)+1)),np.log(list(map(abs, cumulative_regret))),color='red',label='regret cumulatif')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
